# Challenge B103 07/01/2026

## Pitch de l‚Äôexercice üßë‚Äçüè´

>‚å®Ô∏è Pitch de l'exercice :
>
> Installer vCenter et le configurer

[Cours B103.](/RESUME.md#-b103-vmware-esxi--vcenter)

---

## D√©finitions / compr√©hension du syst√®me

- Le Duo VMware

  - ESXi : C'est l'hyperviseur. Il s'installe sur le serveur et fait tourner les VMs.

  - vCenter :  C'est une machine virtuelle compl√®te (bas√©e sur Linux Photon OS) qui contient :

      Une √©norme base de donn√©es (PostgreSQL) pour stocker l'historique de tout ce qui se passe. Des dizaines de services Java (tr√®s gourmands en RAM). Des outils d'analyse, de mise √† jour, de s√©curit√©, etc. C'est con√ßu pour g√©rer 2000 h√¥tes ESXi et 35 000 machines virtuelles. Du coup, m√™me la version "Minuscule" (Tiny) garde cette architecture lourde.

  - Sans lui : Pas de d√©placement de VM √† chaud (vMotion), pas de red√©marrage auto en cas de panne (HA).

- L'Architecture "Poup√©es Russes"

  - On fait de la Virtualisation Imbriqu√©e (Nested) :

    - Niveau 1 : Proxmox (H√¥te principal).

    - Niveau 2 : ESXi (VM dans Proxmox).

    - Niveau 3 : vCenter (VM dans ESXi).

## Cr√©ation de la VM

On va installer une VM Windows 10, pour la rapidit√© et l√©g√®ret√© sur notre serveur.

> - Guest OS : Microsoft Windows / Version : 11/10/2016/2019
> - QEMU agent : Enabled
> - Syst√®me : Graphic card : VirtIO-GPU
> - Disque : ISO Win10 + ISO VirtIO
> - Machine : q35
> - BIOS : "OVMF (UEFI) (+"EFI Disk"")"
> - Disque,Bus/Device,SCSI (plus performant que IDE ou SATA)
> - Taille : 32Go
> - CPU : Cores 2x2
> - Type : host
> - R√©seau : Intel E1000E

## Installation vCenter Stage 1

Pour installer vCenter on monte l'image le disque de la VM et on va lancer installer.exe dans vcsa-ui-installer/win32

![CD](/images/2026-01-07-15-05-35.png)

![install](/images/2026-01-07-15-17-14.png)

![deploy](/images/2026-01-07-15-22-57.png)

On doit se connecter √† la cible (le serveur ESXi)

![target](/images/2026-01-07-15-27-34.png)

On valide les certificats pour avoir une connexion s√©curis√©e (certificat SSL + certificat auto-sign√© ESXi)

![certifs](/images/2026-01-07-15-28-15.png)

On choisit le nom de notre serveur vCenter et un mot de passe complexe

![vCenter](/images/2026-01-07-15-30-31.png)

On choisit la taille du d√©ploiement ici, Tiny (2 CPU, 14 Go de RAM) qui sera suffisant ici pour notre installation.

![d√©ploiement](/images/2026-01-07-15-34-18.png)

Pour le choix du disque "datastore" il faut surtout cocher la case "Enable Thin Disk Mode" (Disque dynamique), sinon, il va r√©server 500 Go d'espace disque d'un coup et saturer notre stockage !

![datastore](/images/2026-01-07-15-51-46.png)

Maintenant on passe aux param√®tres r√©seau, il faut une IP statique, il demande √©galement un nom de domaine, on peut lui mettre notre adresse IP fixe. On ajouter notre subnet en /16, notre default gateway (la pfsense), et DNS.

![network](/images/2026-01-07-15-57-24.png)

On est pr√™t √† lancer l'installation du stage 1 !

![stage 1](/images/2026-01-07-16-01-45.png)

D√©ploiement

![d√©ploiement](/images/2026-01-07-16-03-00.png)

On peut voir l'avancement dans VSXi √©galement

![ESXi](/images/2026-01-07-16-06-10.png)

Successfully deployed

![OK](/images/2026-01-07-16-11-50.png)

## Param√©trage vCenter Stage 2

![stage2](/images/2026-01-07-16-18-35.png)

On choisis Synchronize time with the ESXi host. Si l'heure du vCenter se d√©cale de celle de l'ESXi de plus de 5 minutes, on aura un probl√®me de certificats SSL. En les synchronisant, ils restent toujours synchrones.

![synchro](/images/2026-01-07-16-32-22.png)

Maintenant on va cr√©er le domaine d'administration de VMware (Single Sign-On)

On ne peut pas changer administrator, pour se connecter ce sera donc <administrator@vsphere.local> / password

![SSO](/images/2026-01-07-16-37-33.png)

Ici on peut d√©cocher la CEIP

![CEIP](/images/2026-01-07-16-38-50.png)

Pr√™t

![stage 2](/images/2026-01-07-16-39-22.png)

![warn](/images/2026-01-07-16-40-47.png)

Complete !

![done](/images/2026-01-07-17-24-50.png)

On peut maintenant se connecter √† vSphere via l'interface web <https://10.0.0.70:443>

![vsphere](/images/2026-01-07-17-26-06.png)

![client](/images/2026-01-07-17-29-48.png)

## vSphere Client

On peut cr√©er un nouveau centre de donn√©e (**datacenter**) et y ajouter notre **h√¥te** (l'EXSi)

![ajout](/images/2026-01-08-11-37-34.png)

On ajoute notre utilisateur root pour se connecter √† cet EXSi, on peut voir les VM qui y sont install√©es

![h√¥te](/images/2026-01-08-11-39-08.png)

Ici on va extraire l'image, vCenter "copie" la configuration exacte (version ESXi, pilotes) qui tourne d√©j√† sur notre serveur physique pour en faire le mod√®le de r√©f√©rence.

Pourquoi : Cela √©vite d'√©craser le syst√®me par une version incompatible et garantit que l'h√¥te est import√© tel quel, sans forcer de mise √† jour risqu√©e imm√©diatement.

![image](/images/2026-01-08-11-40-39.png)

Le mode de verrouillage permet de s√©curiser, en tant que pro on passerait en mode Strict, mais on va le laisser d√©sactiv√© en cas de probl√®me pour nous.

![verrouillage](/images/2026-01-08-11-44-42.png)

Tout en bas on a une barre des t√¢ches (comme Proxmox) ou on peut voir les t√¢ches en cours.

![t√¢ches](/images/2026-01-08-11-48-36.png)

Une fois termin√© on a notre **HCI** (Hyper-Converged Infrastructure : Infrastructure Hyper-Converg√©e), qui permet de tout regrouper et g√©rer simultan√©ment.

![HCI](/images/2026-01-08-11-51-34.png)

On va faire de m√™me pour l'autre serveur EXSi, avec les m√™mes configurations.

![second h√¥te](/images/2026-01-08-12-02-06.png)

- *Petit bonus*

On peut se connecter depuis **VMware Workstation Pro** √† notre serveur vSphere pour y avoir acc√®s directement via Workstation, il suffit de faire Fichier :

![co](/images/2026-01-08-13-23-12.png)

![vmwarevsphere](/images/2026-01-08-16-02-26.png)

- *Info*

On peut √©galement migrer (√† froid) nos VM entre les 2 serveurs EXSi sans cr√©er de Cluster dans vSphere.

## Ajout d'un Cluster

On va cr√©er un Cluster dans notre Datacenter

![Cluster](/images/2026-01-08-13-35-54.png)

- vSphere DRS (Distributed Resource Scheduler) : C'est l'√©quilibrage de charge automatique qui d√©place vos VMs d'un serveur surcharg√© vers un serveur libre pour garantir les meilleures performances.

- vSphere HA (High Availability) : C'est la haute disponibilit√© qui red√©marre automatiquement vos VMs sur un autre serveur survivant si leur serveur physique tombe en panne.

- vSAN (Virtual Storage Area Network) : Combine automatiquement les disques locaux de chaque h√¥te ESXi pour cr√©er un seul datastore partag√©.

![Cluster](/images/2026-01-08-13-42-44.png)

On s√©lectionne l'image et on termine la cr√©ation du Cluster.

![Done](/images/2026-01-08-13-46-59.png)

On a maintenant un Cluster, on peut ajouter les h√¥tes avec clic droit ou en les faisant glisser directement, on nous propose 2 options, le pool racine ou tout est au m√™me niveau (plus simple pour un petit lab) ou cr√©er un pool pour garder une arborescence

![d√©placer](/images/2026-01-08-13-48-00.png)

![Cluster](/images/2026-01-08-13-56-22.png)

Les VM sont au m√™me niveau mais on peut toujours voir quel est leur H√¥te

![h√¥te](/images/2026-01-08-14-01-06.png)

Le Mode Maintenance

On va activer le vMotion (fonctionnalit√© VMware) pour la Migration √† chaud, √ßa va se passer au niveau des cartes r√©seau. On va cr√©er un DS et un DSM qui vont g√©rer un r√©seau cluster

![vMotion](/images/2026-01-08-14-02-17.png)

Trafic vMotion, on peut configurer/changer nos IP en statique etc mais on va laisser tel quel

![trafic vmotion](/images/2026-01-08-14-17-04.png)

terme Quorum est souvent utilis√© un peu vite, mais il est fondamental pour la Haute Disponibilit√© (HA).

![trafic](/images/2026-01-08-14-25-02.png)

On finit la configuration et on peut terminer, en allant dans la configuration, on peut voir le commutateur virtuel (DSwitch)

![DSwitch](/images/2026-01-08-14-29-41.png)

Maintenant que le DSwitch est cr√©√© il faut ajouter notre second EXSi dessus, dans la partie r√©seau, clic droit sur DSwitch et Ajouter-g√©rer des h√¥tes

![DSwitch](/images/2026-01-08-16-04-31.png)

![Ajout](/images/2026-01-08-16-05-28.png)

Pour l'adaptateur r√©seau physique, on choisir le m√™me uplink que l'autre serveur (uplink1)

Maintenant on doit assigner l'adaptateur  r√©seau VMkernel √† un groupe de ports (le DSwitch-Management Network)

![vmk0](/images/2026-01-08-16-21-26.png)

On doit migrer la gestion r√©seau des VM et attribuer le port DSwitch-VM Network

![mise reseau VM](/images/2026-01-08-16-28-35.png)

DSwitch Ok pour celui-ci √©galement

![OK](/images/2026-01-08-19-43-48.png)

![OK](/images/2026-01-08-15-03-15.png)

- *Info*

Le vSwitch Distribu√© (DSwitch) est mieux pour la production, mais le vSwitch Standard fonctionne parfaitement pour vMotion. il suffit donc d'activer le service vMotion" sur le switch. Dans Adaptateurs VMkernel > Modifier > Cocher la case vMotion

## Migration √† chaud

On peut directement faire glisser une VM (ici Alpine du 10.0.0.63) sur l'autre Hyperviseur

![move](/images/2026-01-08-19-52-19.png)

![migrer](/images/2026-01-08-19-50-03.png)

On s√©lectionne l'h√¥te cible

![ressource de calcul](/images/2026-01-08-19-52-46.png)

On choisi le stockage cible

![stockage](/images/2026-01-08-20-27-53.png)

On s√©lectionne l'adaptateur r√©seau

![network](/images/2026-01-08-20-28-13.png)

S√©lectionner la priorit√© de vMotion

![priorit√©](/images/2026-01-08-20-28-29.png)

Terminer

![fin](/images/2026-01-08-20-28-51.png)

On peut voir que notre VM a bien √©t√© migr√©e sans interruption !

![OK](/images/2026-01-08-20-29-48.png)
